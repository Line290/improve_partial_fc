{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/train/results/dev/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import time\n",
    "import cPickle\n",
    "# import custom_layers\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvFactory(data, num_filter, kernel, stride=(1, 1), pad=(0, 0), act_type=\"relu\", mirror_attr={}, with_act=True, namepre='', args=None):\n",
    "  if args is None:\n",
    "    weight = mx.sym.Variable(namepre+'_weight')\n",
    "    bias = mx.sym.Variable(namepre+'_bias')\n",
    "    gamma = mx.sym.Variable(namepre+'_gamma')\n",
    "    beta = mx.sym.Variable(namepre+'_beta')\n",
    "    args = {'weight':weight, 'bias':bias}\n",
    "  else:\n",
    "    weight = args['weight']\n",
    "    bias = args['bias']\n",
    "    gamma = args['gamma']\n",
    "    beta = args['beta']\n",
    "  \n",
    "  conv = mx.symbol.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, weight=weight, bias=bias, name=namepre+'_conv')\n",
    "  bn = mx.symbol.BatchNorm(data=conv, gamma=gamma, beta=beta, name=namepre+'_bn')\n",
    "  act = bn\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(data=bn, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def stem(data, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'conv1a_3_3':None, 'conv2a_3_3':None, 'conv2b_3_3':None, 'conv3b_1_1':None, 'conv4a_3_3':None}\n",
    "  conv1a_3_3, args['conv1a_3_3'] = ConvFactory(data=data, num_filter=32,\n",
    "                           kernel=(3, 3), stride=(2, 2), namepre=namepre+'_conv1a_3_3', args=args['conv1a_3_3'])\n",
    "  conv2a_3_3, args['conv2a_3_3'] = ConvFactory(conv1a_3_3, 32, (3, 3), namepre=namepre+'_conv2a_3_3', args=args['conv2a_3_3'])\n",
    "  conv2b_3_3, args['conv2b_3_3'] = ConvFactory(conv2a_3_3, 64, (3, 3), pad=(1, 1), namepre=namepre+'_conv2b_3_3', args=args['conv2b_3_3'])\n",
    "  maxpool3a_3_3 = mx.symbol.Pooling(\n",
    "      data=conv2b_3_3, kernel=(3, 3), stride=(2, 2), pool_type='max', name=namepre+'_maxpool3a_3_3')\n",
    "  conv3b_1_1, args['conv3b_1_1'] = ConvFactory(maxpool3a_3_3, 80, (1, 1), namepre=namepre+'_conv3b_1_1', args=args['conv3b_1_1'])\n",
    "  conv4a_3_3, args['conv4a_3_3'] = ConvFactory(conv3b_1_1, 192, (3, 3), namepre=namepre+'_conv4a_3_3', args=args['conv4a_3_3'])\n",
    "\n",
    "  return conv4a_3_3, args \n",
    "\n",
    "\n",
    "def reductionA(conv4a_3_3, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv2_0':None, 'tower_conv2_1':None, 'tower_conv2_2':None, 'tower_conv3_1':None}\n",
    "  maxpool5a_3_3 = mx.symbol.Pooling(\n",
    "      data=conv4a_3_3, kernel=(3, 3), stride=(2, 2), pool_type='max', name=namepre+'_maxpool5a_3_3')\n",
    "\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(maxpool5a_3_3, 96, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(maxpool5a_3_3, 48, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 64, (5, 5), pad=(2, 2), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "\n",
    "  tower_conv2_0, args['tower_conv2_0'] = ConvFactory(maxpool5a_3_3, 64, (1, 1), namepre=namepre+'_tower_conv2_0', args=args['tower_conv2_0'])\n",
    "  tower_conv2_1, args['tower_conv2_1'] = ConvFactory(tower_conv2_0, 96, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_1', args=args['tower_conv2_1'])\n",
    "  tower_conv2_2, args['tower_conv2_2'] = ConvFactory(tower_conv2_1, 96, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_2', args=args['tower_conv2_2'])\n",
    "\n",
    "  tower_pool3_0 = mx.symbol.Pooling(data=maxpool5a_3_3, kernel=(\n",
    "      3, 3), stride=(1, 1), pad=(1, 1), pool_type='avg', name=namepre+'_tower_pool3_0')\n",
    "  tower_conv3_1, args['tower_conv3_1'] = ConvFactory(tower_pool3_0, 64, (1, 1), namepre=namepre+'_tower_conv3_1', args=args['tower_conv3_1'])\n",
    "  tower_5b_out = mx.symbol.Concat(\n",
    "      *[tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1])\n",
    "  return tower_5b_out, args \n",
    "\n",
    "\n",
    "def reductionB(net, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv1_2':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 384, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 256, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv1_2, args['tower_conv1_2'] = ConvFactory(tower_conv1_1, 384, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv1_2', args=args['tower_conv1_2'])\n",
    "  tower_pool = mx.symbol.Pooling(net, kernel=(\n",
    "      3, 3), stride=(2, 2), pool_type='max', name=namepre+'_tower_pool')\n",
    "  net = mx.symbol.Concat(*[tower_conv, tower_conv1_2, tower_pool])\n",
    "\n",
    "  return net, args\n",
    "\n",
    "\n",
    "def reductionC(net, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv0_1':None, 'tower_conv1':None, 'tower_conv1_1':None, 'tower_conv2':None, 'tower_conv2_1':None, 'tower_conv2_2':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv0_1, args['tower_conv0_1'] = ConvFactory(tower_conv, 384, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv0_1', args=args['tower_conv0_1'])\n",
    "  tower_conv1, args['tower_conv1'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv1', args=args['tower_conv1'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1, 288, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv2, args['tower_conv2'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv2', args=args['tower_conv2'])\n",
    "  tower_conv2_1, args['tower_conv2_1'] = ConvFactory(tower_conv2, 288, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_1', args=args['tower_conv2_1'])\n",
    "  tower_conv2_2, args['tower_conv2_2'] = ConvFactory(tower_conv2_1, 320, (3, 3),  stride=(2, 2), namepre=namepre+'_tower_conv2_2', args=args['tower_conv2_2'])\n",
    "  tower_pool = mx.symbol.Pooling(net, kernel=(3, 3), stride=(2, 2), pool_type='max', name=namepre+'_tower_pool')\n",
    "  net = mx.symbol.Concat(*[tower_conv0_1, tower_conv1_1, tower_conv2_2, tower_pool])\n",
    "  return net, args\n",
    "\n",
    "\n",
    "def block35(net, input_num_channels, scale=1.0, with_act=True, act_type='relu', mirror_attr={}, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv2_0':None, 'tower_conv2_1':None, 'tower_conv2_2':None, 'tower_out':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 32, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 32, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 32, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv2_0, args['tower_conv2_0'] = ConvFactory(net, 32, (1, 1), namepre=namepre+'_tower_conv2_0', args=args['tower_conv2_0'])\n",
    "  tower_conv2_1, args['tower_conv2_1'] = ConvFactory(tower_conv2_0, 48, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_1', args=args['tower_conv2_1'])\n",
    "  tower_conv2_2, args['tower_conv2_2'] = ConvFactory(tower_conv2_1, 64, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_2', args=args['tower_conv2_2'])\n",
    "  tower_mixed = mx.symbol.Concat(*[tower_conv, tower_conv1_1, tower_conv2_2])\n",
    "  tower_out, args['tower_out'] = ConvFactory(\n",
    "      tower_mixed, input_num_channels, (1, 1), with_act=False, namepre=namepre+'_tower_out', args=args['tower_out'])\n",
    "\n",
    "  net = net + scale * tower_out\n",
    "  act = net\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(\n",
    "          data=net, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def block17(net, input_num_channels, scale=1.0, with_act=True, act_type='relu', mirror_attr={}, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv1_2':None, 'tower_out':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 192, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 129, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 160, (1, 7), pad=(1, 2), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv1_2, args['tower_conv1_2'] = ConvFactory(tower_conv1_1, 192, (7, 1), pad=(2, 1), namepre=namepre+'_tower_conv1_2', args=args['tower_conv1_2'])\n",
    "  tower_mixed = mx.symbol.Concat(*[tower_conv, tower_conv1_2])\n",
    "  tower_out, args['tower_out'] = ConvFactory(\n",
    "      tower_mixed, input_num_channels, (1, 1), with_act=False, namepre=namepre+'_tower_out', args=args['tower_out'])\n",
    "  net = net + scale * tower_out\n",
    "  act = net\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(\n",
    "          data=net, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def block8(net, input_num_channels, scale=1.0, with_act=True, act_type='relu', mirror_attr={}, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv1_2':None, 'tower_out':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 192, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 192, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 224, (1, 3), pad=(0, 1), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv1_2, args['tower_conv1_2'] = ConvFactory(tower_conv1_1, 256, (3, 1), pad=(1, 0), namepre=namepre+'_tower_conv1_2', args=args['tower_conv1_2'])\n",
    "  tower_mixed = mx.symbol.Concat(*[tower_conv, tower_conv1_2])\n",
    "  tower_out, args['tower_out'] = ConvFactory(\n",
    "      tower_mixed, input_num_channels, (1, 1), with_act=False, namepre=namepre+'_tower_out', args=args['tower_out'])\n",
    "  net = net + scale * tower_out\n",
    "  act = net\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(\n",
    "          data=net, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def repeat(inputs, repetitions, layer, *ltargs, **kwargs):\n",
    "  outputs = inputs\n",
    "  namepre = kwargs['namepre']\n",
    "  args = kwargs['args']\n",
    "  if args is None:\n",
    "    args = {}\n",
    "    for i in xrange(repetitions):\n",
    "      argname='repeat_'+str(i)\n",
    "      args[argname] = None\n",
    "  for i in range(repetitions):\n",
    "    kwargs['namepre'] = namepre+'_'+str(i)\n",
    "    argname='repeat_'+str(i)\n",
    "    kwargs['args'] = args[argname]\n",
    "#    print ltargs\n",
    "#    print kwargs\n",
    "    outputs, args[argname] = layer(outputs, *ltargs, **kwargs)\n",
    "\n",
    "  return outputs, args\n",
    "\n",
    "\n",
    "def create_inception_resnet_v2(data, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'stem':None, 'reductionA':None, 'repeat_block35':None, 'reductionB':None, \n",
    "            'repeat_block17':None, 'reductionC':None, 'repeat_block8':None, \n",
    "            'final_block8':None, 'final_conv':None, 'finalfc':None}\n",
    "\n",
    "  stem_net, args['stem']= stem(data, namepre=namepre+'_stem', args=args['stem'])\n",
    "\n",
    "  reduceA, args['reductionA'] = reductionA(stem_net, namepre=namepre+'_reductionA', args=args['reductionA'])\n",
    "\n",
    "  repeat_block35, args['repeat_block35'] = repeat(reduceA, 2, block35, scale=0.17, input_num_channels=320, namepre=namepre+'_repeat_block35', args=args['repeat_block35'])\n",
    "\n",
    "\n",
    "  reduceB, args['reductionB'] = reductionB(repeat_block35, namepre=namepre+'_reductionB', args=args['reductionB'])\n",
    "\n",
    "  repeat_block17, args['repeat_block17'] = repeat(reduceB, 4, block17, scale=0.1, input_num_channels=1088, namepre=namepre+'_repeat_block17', args=args['repeat_block17'])\n",
    "\n",
    "  reduceC, args['reductionC'] = reductionC(repeat_block17, namepre=namepre+'_reductionC', args=args['reductionC'])\n",
    "\n",
    "  repeat_block8, args['repeat_block8'] = repeat(reduceC, 2, block8, scale=0.2, input_num_channels=2080, namepre=namepre+'_repeat_block8', args=args['repeat_block8'])\n",
    "  final_block8, args['final_block8'] = block8(repeat_block8, with_act=False, input_num_channels=2080, namepre=namepre+'_final_block8', args=args['final_block8'])\n",
    "\n",
    "  final_conv, args['final_conv'] = ConvFactory(final_block8, 1536, (1, 1), namepre=namepre+'_final_conv', args=args['final_conv'])\n",
    "  final_pool = mx.symbol.Pooling(final_conv, kernel=(8, 8), global_pool=True, pool_type='avg', name=namepre+'_final_pool')\n",
    "#   final_pool = mx.symbol.Pooling(final_conv, kernel=(5, 5), stride=(1, 1), pool_type='avg', name=namepre+'_final_pool')\n",
    "  final_flatten = mx.symbol.Flatten(final_pool, name=namepre+'_final_flatten')\n",
    "\n",
    "  drop1 = mx.sym.Dropout(data=final_flatten, p=0.5, name=namepre+'_dropout1')\n",
    "\n",
    "  if args['finalfc'] is None:\n",
    "    args['finalfc'] = {}\n",
    "    args['finalfc']['weight'] = mx.sym.Variable(namepre+'_fc1_weight')\n",
    "    args['finalfc']['bias'] = mx.sym.Variable(namepre+'_fc1_bias')\n",
    "    \n",
    "  reid_fc1 = mx.sym.FullyConnected(data=drop1, num_hidden=512, name=namepre+\"_fc1\", \n",
    "                                   weight=args['finalfc']['weight'], bias=args['finalfc']['bias']) \n",
    "#  reid_act = mx.sym.Activation(data=reid_fc1, act_type='tanh', name=namepre+'_fc1_relu')\n",
    "\n",
    "  net = reid_fc1\n",
    "#  net = final_flatten\n",
    "\n",
    "  return net, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net(data, hardratio, radius):\n",
    "#     data = mx.sym.Variable('data')\n",
    "    args_all = None\n",
    "    feat_final, args_all = create_inception_resnet_v2(data, namepre='part1', args=args_all)\n",
    "    feat_final = mx.sym.BatchNorm(data=feat_final, fix_gamma=False, name='feat_bn1')\n",
    "\n",
    "\n",
    "    min_value =10**-36\n",
    "    norm_value = radius\n",
    "    #  norm_value = 24\n",
    "    logging.info('norm_value:%f, min_value:%e, hardratio:%f', norm_value, min_value, hardratio)\n",
    "\n",
    "    #norm\n",
    "    znorm_loss = None\n",
    "    if norm_value>0:\n",
    "    #    proxy_Z = mx.sym.L2Normalization(proxy_Z) * norm_value\n",
    "    #    feat_final = mx.sym.L2Normalization(feat_final) * norm_value\n",
    "#         proxy_Znorm = mx.sym.sum_axis(proxy_Z**2, axis=1)\n",
    "#         proxy_Znorm = mx.sym.sqrt(proxy_Znorm) + min_value\n",
    "\n",
    "#     #    znorm_loss = mx.sym.abs(proxy_Znorm - 1.0)\n",
    "#     #    znorm_loss = mx.sym.sum(znorm_loss)\n",
    "#     #    znorm_loss = mx.sym.MakeLoss(znorm_loss)\n",
    "\n",
    "#         proxy_Znorm = mx.sym.Reshape(proxy_Znorm, shape=(-2, 1))\n",
    "#         proxy_Z = mx.sym.broadcast_div(proxy_Z, proxy_Znorm)# * norm_value\n",
    "\n",
    "        feat_finalnorm = mx.sym.sum_axis(feat_final**2, axis=1)\n",
    "        feat_finalnorm = mx.sym.sqrt(feat_finalnorm) + min_value\n",
    "        feat_finalnorm = mx.sym.Reshape(feat_finalnorm, shape=(-2, 1))\n",
    "        feat_final = mx.sym.broadcast_div(feat_final, feat_finalnorm) * norm_value\n",
    "#     X = mx.nd.empty(shape=(64, 10000000), ctx=mx.cpu(0))\n",
    "    return feat_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "batchsize = 64\n",
    "data_shape = (batchsize, 3, 240, 120)\n",
    "hardratio = 0.0001\n",
    "radius = 0\n",
    "lr = 0.01\n",
    "beta1 = 0.9\n",
    "ctx = mx.gpu(0)\n",
    "part_net = create_net(data, hardratio, radius)\n",
    "mod = mx.mod.Module(symbol=part_net, data_names=(\"data\",), context=ctx)\n",
    "mod.bind(data_shapes=[('data', data_shape)], inputs_need_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64L, 3L, 240L, 120L)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_batch = mx.random.normal(0, 1.0, shape=data_shape)\n",
    "print data_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208]\n",
      "lr_start:1.0e+00, lr_min:1.0e-05, lr_reduce:0.40, lr_stepsnum:13\n"
     ]
    }
   ],
   "source": [
    "lr_start = 1 #0.2 #0.1 #0.02 #0.05 #0.57 \n",
    "#lr_min = 10**-5\n",
    "lr_min = 10**-5\n",
    "lr_reduce = 0.4 #0.99\n",
    "lr_stepnum = np.log(lr_min/lr_start)/np.log(lr_reduce)\n",
    "lr_stepnum = np.int(np.ceil(lr_stepnum))\n",
    "dlr = 1050/batchsize\n",
    "dlr_steps = [dlr*i for i in xrange(1, lr_stepnum+1)]\n",
    "print dlr_steps\n",
    "print 'lr_start:%.1e, lr_min:%.1e, lr_reduce:%.2f, lr_stepsnum:%d'%(lr_start, lr_min, lr_reduce, lr_stepnum)\n",
    "lr_scheduler = mx.lr_scheduler.MultiFactorScheduler(dlr_steps, lr_reduce)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'base_lr',\n",
       " 'count',\n",
       " 'cur_step_ind',\n",
       " 'factor',\n",
       " 'step']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_schedule(schedule_fn, iterations=1500):\n",
    "    # Iteration count starting at 1\n",
    "    iterations = [i+1 for i in range(iterations)]\n",
    "    lrs = [schedule_fn(i) for i in iterations]\n",
    "    plt.scatter(iterations, lrs)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr_scheduler.base_lr = 1\n",
    "plot_schedule(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.init_params(mx.init.Xavier(factor_type=\"in\", magnitude=2.34))\n",
    "mod.init_optimizer(optimizer=\"adam\", \n",
    "                   optimizer_params={\n",
    "                       \"learning_rate\": lr_start,\n",
    "                       'lr_scheduler':lr_scheduler,\n",
    "                       'clip_gradient':None,\n",
    "                       \"wd\": 0.0005,\n",
    "                       \"beta1\": beta1,\n",
    "                   })\n",
    "\n",
    "# mod._optimizer.lr_mult = [0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.forward(mx.io.DataBatch([data_batch,]), is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = mx.random.normal(0, 1.0, shape=(64, 512))\n",
    "mod.backward([diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod._optimizer.num_update = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "1 1.0\n",
      "2 1.0\n",
      "3 1.0\n",
      "4 1.0\n",
      "5 1.0\n",
      "6 1.0\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "17 0.4\n",
      "18 0.4\n",
      "19 0.4\n",
      "20 0.4\n",
      "21 0.4\n",
      "22 0.4\n",
      "23 0.4\n",
      "24 0.4\n",
      "25 0.4\n",
      "26 0.4\n",
      "27 0.4\n",
      "28 0.4\n",
      "29 0.4\n",
      "30 0.4\n",
      "31 0.4\n",
      "32 0.4\n",
      "33 0.16\n",
      "34 0.16\n",
      "35 0.16\n",
      "36 0.16\n",
      "37 0.16\n",
      "38 0.16\n",
      "39 0.16\n",
      "40 0.16\n",
      "41 0.16\n",
      "42 0.16\n",
      "43 0.16\n",
      "44 0.16\n",
      "45 0.16\n",
      "46 0.16\n",
      "47 0.16\n",
      "48 0.16\n",
      "49 0.064\n",
      "50 0.064\n",
      "51 0.064\n",
      "52 0.064\n",
      "53 0.064\n",
      "54 0.064\n",
      "55 0.064\n",
      "56 0.064\n",
      "57 0.064\n",
      "58 0.064\n",
      "59 0.064\n",
      "60 0.064\n",
      "61 0.064\n",
      "62 0.064\n",
      "63 0.064\n",
      "64 0.064\n",
      "65 0.0256\n",
      "66 0.0256\n",
      "67 0.0256\n",
      "68 0.0256\n",
      "69 0.0256\n",
      "70 0.0256\n",
      "71 0.0256\n",
      "72 0.0256\n",
      "73 0.0256\n",
      "74 0.0256\n",
      "75 0.0256\n",
      "76 0.0256\n",
      "77 0.0256\n",
      "78 0.0256\n",
      "79 0.0256\n",
      "80 0.0256\n",
      "81 0.01024\n",
      "82 0.01024\n",
      "83 0.01024\n",
      "84 0.01024\n",
      "85 0.01024\n",
      "86 0.01024\n",
      "87 0.01024\n",
      "88 0.01024\n",
      "89 0.01024\n",
      "90 0.01024\n",
      "91 0.01024\n",
      "92 0.01024\n",
      "93 0.01024\n",
      "94 0.01024\n",
      "95 0.01024\n",
      "96 0.01024\n",
      "97 0.004096\n",
      "98 0.004096\n",
      "99 0.004096\n",
      "100 0.004096\n",
      "101 0.004096\n",
      "102 0.004096\n",
      "103 0.004096\n",
      "104 0.004096\n",
      "105 0.004096\n",
      "106 0.004096\n",
      "107 0.004096\n",
      "108 0.004096\n",
      "109 0.004096\n",
      "110 0.004096\n",
      "111 0.004096\n",
      "112 0.004096\n",
      "113 0.0016384\n",
      "114 0.0016384\n",
      "115 0.0016384\n",
      "116 0.0016384\n",
      "117 0.0016384\n",
      "118 0.0016384\n",
      "119 0.0016384\n",
      "120 0.0016384\n",
      "121 0.0016384\n",
      "122 0.0016384\n",
      "123 0.0016384\n",
      "124 0.0016384\n",
      "125 0.0016384\n",
      "126 0.0016384\n",
      "127 0.0016384\n",
      "128 0.0016384\n",
      "129 0.00065536\n",
      "130 0.00065536\n",
      "131 0.00065536\n",
      "132 0.00065536\n",
      "133 0.00065536\n",
      "134 0.00065536\n",
      "135 0.00065536\n",
      "136 0.00065536\n",
      "137 0.00065536\n",
      "138 0.00065536\n",
      "139 0.00065536\n",
      "140 0.00065536\n",
      "141 0.00065536\n",
      "142 0.00065536\n",
      "143 0.00065536\n",
      "144 0.00065536\n",
      "145 0.000262144\n",
      "146 0.000262144\n",
      "147 0.000262144\n",
      "148 0.000262144\n",
      "149 0.000262144\n",
      "150 0.000262144\n",
      "151 0.000262144\n",
      "152 0.000262144\n",
      "153 0.000262144\n",
      "154 0.000262144\n",
      "155 0.000262144\n",
      "156 0.000262144\n",
      "157 0.000262144\n",
      "158 0.000262144\n",
      "159 0.000262144\n",
      "160 0.000262144\n",
      "161 0.0001048576\n",
      "162 0.0001048576\n",
      "163 0.0001048576\n",
      "164 0.0001048576\n",
      "165 0.0001048576\n",
      "166 0.0001048576\n",
      "167 0.0001048576\n",
      "168 0.0001048576\n",
      "169 0.0001048576\n",
      "170 0.0001048576\n",
      "171 0.0001048576\n",
      "172 0.0001048576\n",
      "173 0.0001048576\n",
      "174 0.0001048576\n",
      "175 0.0001048576\n",
      "176 0.0001048576\n",
      "177 4.194304e-05\n",
      "178 4.194304e-05\n",
      "179 4.194304e-05\n",
      "180 4.194304e-05\n",
      "181 4.194304e-05\n",
      "182 4.194304e-05\n",
      "183 4.194304e-05\n",
      "184 4.194304e-05\n",
      "185 4.194304e-05\n",
      "186 4.194304e-05\n",
      "187 4.194304e-05\n",
      "188 4.194304e-05\n",
      "189 4.194304e-05\n",
      "190 4.194304e-05\n",
      "191 4.194304e-05\n",
      "192 4.194304e-05\n",
      "193 1.6777216e-05\n",
      "194 1.6777216e-05\n",
      "195 1.6777216e-05\n",
      "196 1.6777216e-05\n",
      "197 1.6777216e-05\n",
      "198 1.6777216e-05\n",
      "199 1.6777216e-05\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    mod.forward(mx.io.DataBatch([data_batch,]), is_train=True)\n",
    "    diff = mx.random.normal(0, 1.0, shape=(64, 512))\n",
    "    mod.backward([diff])\n",
    "    \n",
    "    print mod._optimizer.num_update, mod._optimizer._get_lr(mod._optimizer.num_update)\n",
    "#     print mod._optimizer.num_update, mod._optimizer.lr\n",
    "    \n",
    "    mod.update()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mod.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_get_lr',\n",
       " '_get_wd',\n",
       " '_index_update_count',\n",
       " '_update_count',\n",
       " 'begin_num_update',\n",
       " 'beta1',\n",
       " 'beta2',\n",
       " 'clip_gradient',\n",
       " 'create_optimizer',\n",
       " 'create_state',\n",
       " 'create_state_multi_precision',\n",
       " 'epsilon',\n",
       " 'idx2name',\n",
       " 'lazy_update',\n",
       " 'learning_rate',\n",
       " 'lr',\n",
       " 'lr_mult',\n",
       " 'lr_scheduler',\n",
       " 'multi_precision',\n",
       " 'num_update',\n",
       " 'opt_registry',\n",
       " 'param_dict',\n",
       " 'register',\n",
       " 'rescale_grad',\n",
       " 'set_learning_rate',\n",
       " 'set_lr_mult',\n",
       " 'set_lr_scale',\n",
       " 'set_wd_mult',\n",
       " 'sym_info',\n",
       " 'update',\n",
       " 'update_multi_precision',\n",
       " 'wd',\n",
       " 'wd_mult']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mod._optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{}\n",
      "1.0\n",
      "0.0005\n",
      "1e-08\n",
      "201\n",
      "0.9\n",
      "<mxnet.lr_scheduler.MultiFactorScheduler object at 0x7f30dfbdb310>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print mod._optimizer.lr\n",
    "print mod._optimizer.lr_mult\n",
    "print mod._optimizer._get_lr(100)\n",
    "print mod._optimizer.wd\n",
    "print mod._optimizer.epsilon\n",
    "print mod._optimizer.num_update\n",
    "print mod._optimizer.beta1\n",
    "print mod._optimizer.lr_scheduler\n",
    "print mod._optimizer._update_count(mod._optimizer.num_update)\n",
    "# print mod._optimizer.create_state(2, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_get_lr',\n",
       " '_get_wd',\n",
       " '_update_count',\n",
       " 'create_optimizer',\n",
       " 'create_state',\n",
       " 'create_state_multi_precision',\n",
       " 'learning_rate',\n",
       " 'opt_registry',\n",
       " 'register',\n",
       " 'set_learning_rate',\n",
       " 'set_lr_mult',\n",
       " 'set_lr_scale',\n",
       " 'set_wd_mult',\n",
       " 'update',\n",
       " 'update_multi_precision']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mx.optimizer.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59400594 -0.18787093 -1.1587477  ... -0.3922908  -0.32442293\n",
      "   0.40793994]\n",
      " [-0.8142539  -0.04978066  0.24333796 ... -0.02871937  0.32941356\n",
      "  -0.76062626]\n",
      " [ 0.35996005 -0.80182177 -0.32823408 ... -0.8045763   0.09924796\n",
      "   0.2688434 ]\n",
      " ...\n",
      " [ 0.81386065  0.95175695 -1.1954898  ...  0.45502654 -0.48312974\n",
      "   0.7815472 ]\n",
      " [ 0.05001164 -1.193234    0.7274514  ...  0.5062809  -1.1382914\n",
      "  -1.5131106 ]\n",
      " [-0.04340986  1.8692598   0.6630346  ...  0.7136798  -0.01621716\n",
      "  -1.2960556 ]]\n"
     ]
    }
   ],
   "source": [
    "print out[0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         proxy_Znorm = mx.sym.sum_axis(proxy_Z**2, axis=1)\n",
    "#         proxy_Znorm = mx.sym.sqrt(proxy_Znorm) + min_value\n",
    "\n",
    "#     #    znorm_loss = mx.sym.abs(proxy_Znorm - 1.0)\n",
    "#     #    znorm_loss = mx.sym.sum(znorm_loss)\n",
    "#     #    znorm_loss = mx.sym.MakeLoss(znorm_loss)\n",
    "\n",
    "#         proxy_Znorm = mx.sym.Reshape(proxy_Znorm, shape=(-2, 1))\n",
    "#         proxy_Z = mx.sym.broadcast_div(proxy_Z, proxy_Znorm)# * norm_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__array_priority__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__idiv__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '_compose',\n",
       " '_get_ndarray_inputs',\n",
       " '_infer_shape_impl',\n",
       " '_set_attr',\n",
       " '_set_handle',\n",
       " 'abs',\n",
       " 'arccos',\n",
       " 'arccosh',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmax_channel',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_in_context',\n",
       " 'asnumpy',\n",
       " 'asscalar',\n",
       " 'astype',\n",
       " 'attr',\n",
       " 'attr_dict',\n",
       " 'backward',\n",
       " 'bind',\n",
       " 'broadcast_axes',\n",
       " 'broadcast_to',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'clip',\n",
       " 'copy',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'debug_str',\n",
       " 'degrees',\n",
       " 'detach',\n",
       " 'eval',\n",
       " 'exp',\n",
       " 'expand_dims',\n",
       " 'expm1',\n",
       " 'fix',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'floor',\n",
       " 'get_children',\n",
       " 'get_internals',\n",
       " 'gradient',\n",
       " 'handle',\n",
       " 'infer_shape',\n",
       " 'infer_shape_partial',\n",
       " 'infer_type',\n",
       " 'list_arguments',\n",
       " 'list_attr',\n",
       " 'list_auxiliary_states',\n",
       " 'list_inputs',\n",
       " 'list_outputs',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'log_softmax',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'name',\n",
       " 'nanprod',\n",
       " 'nansum',\n",
       " 'norm',\n",
       " 'one_hot',\n",
       " 'ones_like',\n",
       " 'pad',\n",
       " 'pick',\n",
       " 'prod',\n",
       " 'radians',\n",
       " 'rcbrt',\n",
       " 'reciprocal',\n",
       " 'relu',\n",
       " 'repeat',\n",
       " 'reshape',\n",
       " 'reshape_like',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'rsqrt',\n",
       " 'save',\n",
       " 'sigmoid',\n",
       " 'sign',\n",
       " 'simple_bind',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'slice',\n",
       " 'slice_axis',\n",
       " 'slice_like',\n",
       " 'softmax',\n",
       " 'sort',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squeeze',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'take',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tile',\n",
       " 'tojson',\n",
       " 'topk',\n",
       " 'transpose',\n",
       " 'trunc',\n",
       " 'wait_to_read',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_forward(operator.CustomOp):\n",
    "    def __init__(self, hardratio, handle):\n",
    "        self.hardratio = float(hardratio)\n",
    "        self.handle = handle\n",
    "    def forward(self, is_train, req, in_data, out_data, aux):\n",
    "        '''\n",
    "            in_data: [feat_final, label, proxy_Z]\n",
    "            \n",
    "        '''\n",
    "        X = in_data[0].asnumpy()\n",
    "        y_true = in_data[1].asnumpy().reshape(-1,1).astype(np.int32)\n",
    "        W = in_data[2]\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        M, C = W.shape\n",
    "        \n",
    "        self.score = np.empty((N, M), np.float32)\n",
    "        def stream_cal_in_GPU()        \n",
    "            cublasxt.cublasXtSgemm(self.handle,\n",
    "                                   'N', 'N', \n",
    "                                   N, M, C, np.float32(1.0),\n",
    "                                   X.ctypes.data, N, W.ctypes.data, C, np.float32(0.0),\n",
    "                                   self.score.ctypes.data, N)\n",
    "        cublasxt.cublasXtSetCpuRoutine(handle, 0, 0, stream_cal_in_GPU())\n",
    "        # add hardratio\n",
    "        self.score[range(N), y_true.reshape(-1)] += self.hardratio\n",
    "        self.score = self.score - np.max(self.score, axis=1, keepdims=True)\n",
    "        # exp_score = score\n",
    "        self.score = np.exp(self.score)\n",
    "        # probs = score\n",
    "        self.score = self.score / np.sum(self.score, axis=1, keepdims=True)\n",
    "        \n",
    "        K = 999\n",
    "        y_true_score = self.score[range(N), y_true.reshape(-1)]\n",
    "        self.score[range(N), y_true.reshape(-1)] = self.score.min(axis = 1)\n",
    "        TopK_idx = np.argpartition(self.score, -K, axis=1)[:, -K:]\n",
    "        self.score[range(N), y_true.reshape(-1)] = y_true_score\n",
    "        self.TopK_idx = np.hstack((TopK_idx, y_true))\n",
    "        \n",
    "        TopK_score = self.score[np.array(range(N)).reshape(-1,1), self.TopK_idx]\n",
    "#         y_true_reform = np.array([K]*N).reshape(-1,1)\n",
    "        \n",
    "        self.assign(out_data[0], req[0], mx.nd.array(TopK_score))\n",
    "\n",
    "    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n",
    "        d_TopK_score = out_grad[0].asnumpy()\n",
    "        self.score[...] = 0\n",
    "        N, M = self.score.shape\n",
    "        self.score[np.array(range(N)).reshape(-1,1), self.TopK_idx] = d_TopK_score\n",
    "        d_score = mx.nd.array(self.score, dtype='float32')\n",
    "        csr_d_score = d_score.tostype('csr')\n",
    "        \n",
    "        feat_final = in_data[0]\n",
    "        proxy_Z = mx.nd.array(in_data[2], dtype='float32')\n",
    "        self.assign(in_grad[0], req[0], out_grad[0])\n",
    "\n",
    "@operator.register(\"Matrix2DPlusCol\")\n",
    "class Matrix2DPlusCol_Prop(operator.CustomOpProp):\n",
    "    def __init__(self, hardratio=0.0):\n",
    "        super(Matrix2DPlusCol_Prop, self).__init__(need_top_grad=True)\n",
    "        self.hardratio = hardratio\n",
    "    \n",
    "    def list_arguments(self):\n",
    "        return ['data', 'label']\n",
    "\n",
    "    def list_outputs(self):\n",
    "        return ['output']\n",
    "\n",
    "    def infer_shape(self, inshape):\n",
    "        assert len(inshape[0])==2, 'only support 2D matrix...'\n",
    "        data_shape = inshape[0]\n",
    "        label_shape = inshape[1]\n",
    "        return [data_shape, label_shape], [data_shape], []\n",
    "\n",
    "    def create_operator(self, ctx, shapes, dtypes):\n",
    "        return Matrix2DPlusCol(self.hardratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc():\n",
    "    def __init__(self, a=1):\n",
    "        self.b = 2\n",
    "        \n",
    "    def nc(self):\n",
    "        self.c = 1\n",
    "        return self.c\n",
    "    def nb(self):\n",
    "        self.b = 2\n",
    "        self.c = 2\n",
    "        return self.b, self.c\n",
    "    def nbc(self):\n",
    "        return self.b+self.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print a.nc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "print a.nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print a.nbc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/train/results/dev/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import time\n",
    "import cPickle\n",
    "# import custom_layers\n",
    "import logging\n",
    "\n",
    "import skcuda.cublasxt as cublasxt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64L, 3L, 240L, 120L)\n"
     ]
    }
   ],
   "source": [
    "batchsize = 64\n",
    "data_shape = (batchsize, 3, 240, 120)\n",
    "data_batch = mx.random.normal(0, 1.0, shape=data_shape)\n",
    "data_label = mx.nd.array(range(64), dtype='int32')\n",
    "DataIter = mx.io.DataBatch(data=[data_batch], label=[data_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvFactory(data, num_filter, kernel, stride=(1, 1), pad=(0, 0), act_type=\"relu\", mirror_attr={}, with_act=True, namepre='', args=None):\n",
    "  if args is None:\n",
    "    weight = mx.sym.Variable(namepre+'_weight')\n",
    "    bias = mx.sym.Variable(namepre+'_bias')\n",
    "    gamma = mx.sym.Variable(namepre+'_gamma')\n",
    "    beta = mx.sym.Variable(namepre+'_beta')\n",
    "    args = {'weight':weight, 'bias':bias}\n",
    "  else:\n",
    "    weight = args['weight']\n",
    "    bias = args['bias']\n",
    "    gamma = args['gamma']\n",
    "    beta = args['beta']\n",
    "  \n",
    "  conv = mx.symbol.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, weight=weight, bias=bias, name=namepre+'_conv')\n",
    "  bn = mx.symbol.BatchNorm(data=conv, gamma=gamma, beta=beta, name=namepre+'_bn')\n",
    "  act = bn\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(data=bn, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def stem(data, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'conv1a_3_3':None, 'conv2a_3_3':None, 'conv2b_3_3':None, 'conv3b_1_1':None, 'conv4a_3_3':None}\n",
    "  conv1a_3_3, args['conv1a_3_3'] = ConvFactory(data=data, num_filter=32,\n",
    "                           kernel=(3, 3), stride=(2, 2), namepre=namepre+'_conv1a_3_3', args=args['conv1a_3_3'])\n",
    "  conv2a_3_3, args['conv2a_3_3'] = ConvFactory(conv1a_3_3, 32, (3, 3), namepre=namepre+'_conv2a_3_3', args=args['conv2a_3_3'])\n",
    "  conv2b_3_3, args['conv2b_3_3'] = ConvFactory(conv2a_3_3, 64, (3, 3), pad=(1, 1), namepre=namepre+'_conv2b_3_3', args=args['conv2b_3_3'])\n",
    "  maxpool3a_3_3 = mx.symbol.Pooling(\n",
    "      data=conv2b_3_3, kernel=(3, 3), stride=(2, 2), pool_type='max', name=namepre+'_maxpool3a_3_3')\n",
    "  conv3b_1_1, args['conv3b_1_1'] = ConvFactory(maxpool3a_3_3, 80, (1, 1), namepre=namepre+'_conv3b_1_1', args=args['conv3b_1_1'])\n",
    "  conv4a_3_3, args['conv4a_3_3'] = ConvFactory(conv3b_1_1, 192, (3, 3), namepre=namepre+'_conv4a_3_3', args=args['conv4a_3_3'])\n",
    "\n",
    "  return conv4a_3_3, args \n",
    "\n",
    "\n",
    "def reductionA(conv4a_3_3, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv2_0':None, 'tower_conv2_1':None, 'tower_conv2_2':None, 'tower_conv3_1':None}\n",
    "  maxpool5a_3_3 = mx.symbol.Pooling(\n",
    "      data=conv4a_3_3, kernel=(3, 3), stride=(2, 2), pool_type='max', name=namepre+'_maxpool5a_3_3')\n",
    "\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(maxpool5a_3_3, 96, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(maxpool5a_3_3, 48, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 64, (5, 5), pad=(2, 2), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "\n",
    "  tower_conv2_0, args['tower_conv2_0'] = ConvFactory(maxpool5a_3_3, 64, (1, 1), namepre=namepre+'_tower_conv2_0', args=args['tower_conv2_0'])\n",
    "  tower_conv2_1, args['tower_conv2_1'] = ConvFactory(tower_conv2_0, 96, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_1', args=args['tower_conv2_1'])\n",
    "  tower_conv2_2, args['tower_conv2_2'] = ConvFactory(tower_conv2_1, 96, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_2', args=args['tower_conv2_2'])\n",
    "\n",
    "  tower_pool3_0 = mx.symbol.Pooling(data=maxpool5a_3_3, kernel=(\n",
    "      3, 3), stride=(1, 1), pad=(1, 1), pool_type='avg', name=namepre+'_tower_pool3_0')\n",
    "  tower_conv3_1, args['tower_conv3_1'] = ConvFactory(tower_pool3_0, 64, (1, 1), namepre=namepre+'_tower_conv3_1', args=args['tower_conv3_1'])\n",
    "  tower_5b_out = mx.symbol.Concat(\n",
    "      *[tower_conv, tower_conv1_1, tower_conv2_2, tower_conv3_1])\n",
    "  return tower_5b_out, args \n",
    "\n",
    "\n",
    "def reductionB(net, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv1_2':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 384, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 256, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv1_2, args['tower_conv1_2'] = ConvFactory(tower_conv1_1, 384, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv1_2', args=args['tower_conv1_2'])\n",
    "  tower_pool = mx.symbol.Pooling(net, kernel=(\n",
    "      3, 3), stride=(2, 2), pool_type='max', name=namepre+'_tower_pool')\n",
    "  net = mx.symbol.Concat(*[tower_conv, tower_conv1_2, tower_pool])\n",
    "\n",
    "  return net, args\n",
    "\n",
    "\n",
    "def reductionC(net, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv0_1':None, 'tower_conv1':None, 'tower_conv1_1':None, 'tower_conv2':None, 'tower_conv2_1':None, 'tower_conv2_2':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv0_1, args['tower_conv0_1'] = ConvFactory(tower_conv, 384, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv0_1', args=args['tower_conv0_1'])\n",
    "  tower_conv1, args['tower_conv1'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv1', args=args['tower_conv1'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1, 288, (3, 3), stride=(2, 2), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv2, args['tower_conv2'] = ConvFactory(net, 256, (1, 1), namepre=namepre+'_tower_conv2', args=args['tower_conv2'])\n",
    "  tower_conv2_1, args['tower_conv2_1'] = ConvFactory(tower_conv2, 288, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_1', args=args['tower_conv2_1'])\n",
    "  tower_conv2_2, args['tower_conv2_2'] = ConvFactory(tower_conv2_1, 320, (3, 3),  stride=(2, 2), namepre=namepre+'_tower_conv2_2', args=args['tower_conv2_2'])\n",
    "  tower_pool = mx.symbol.Pooling(net, kernel=(3, 3), stride=(2, 2), pool_type='max', name=namepre+'_tower_pool')\n",
    "  net = mx.symbol.Concat(*[tower_conv0_1, tower_conv1_1, tower_conv2_2, tower_pool])\n",
    "  return net, args\n",
    "\n",
    "\n",
    "def block35(net, input_num_channels, scale=1.0, with_act=True, act_type='relu', mirror_attr={}, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv2_0':None, 'tower_conv2_1':None, 'tower_conv2_2':None, 'tower_out':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 32, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 32, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 32, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv2_0, args['tower_conv2_0'] = ConvFactory(net, 32, (1, 1), namepre=namepre+'_tower_conv2_0', args=args['tower_conv2_0'])\n",
    "  tower_conv2_1, args['tower_conv2_1'] = ConvFactory(tower_conv2_0, 48, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_1', args=args['tower_conv2_1'])\n",
    "  tower_conv2_2, args['tower_conv2_2'] = ConvFactory(tower_conv2_1, 64, (3, 3), pad=(1, 1), namepre=namepre+'_tower_conv2_2', args=args['tower_conv2_2'])\n",
    "  tower_mixed = mx.symbol.Concat(*[tower_conv, tower_conv1_1, tower_conv2_2])\n",
    "  tower_out, args['tower_out'] = ConvFactory(\n",
    "      tower_mixed, input_num_channels, (1, 1), with_act=False, namepre=namepre+'_tower_out', args=args['tower_out'])\n",
    "\n",
    "  net = net + scale * tower_out\n",
    "  act = net\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(\n",
    "          data=net, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def block17(net, input_num_channels, scale=1.0, with_act=True, act_type='relu', mirror_attr={}, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv1_2':None, 'tower_out':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 192, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 129, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 160, (1, 7), pad=(1, 2), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv1_2, args['tower_conv1_2'] = ConvFactory(tower_conv1_1, 192, (7, 1), pad=(2, 1), namepre=namepre+'_tower_conv1_2', args=args['tower_conv1_2'])\n",
    "  tower_mixed = mx.symbol.Concat(*[tower_conv, tower_conv1_2])\n",
    "  tower_out, args['tower_out'] = ConvFactory(\n",
    "      tower_mixed, input_num_channels, (1, 1), with_act=False, namepre=namepre+'_tower_out', args=args['tower_out'])\n",
    "  net = net + scale * tower_out\n",
    "  act = net\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(\n",
    "          data=net, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def block8(net, input_num_channels, scale=1.0, with_act=True, act_type='relu', mirror_attr={}, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'tower_conv':None, 'tower_conv1_0':None, 'tower_conv1_1':None, 'tower_conv1_2':None, 'tower_out':None}\n",
    "  tower_conv, args['tower_conv'] = ConvFactory(net, 192, (1, 1), namepre=namepre+'_tower_conv', args=args['tower_conv'])\n",
    "  tower_conv1_0, args['tower_conv1_0'] = ConvFactory(net, 192, (1, 1), namepre=namepre+'_tower_conv1_0', args=args['tower_conv1_0'])\n",
    "  tower_conv1_1, args['tower_conv1_1'] = ConvFactory(tower_conv1_0, 224, (1, 3), pad=(0, 1), namepre=namepre+'_tower_conv1_1', args=args['tower_conv1_1'])\n",
    "  tower_conv1_2, args['tower_conv1_2'] = ConvFactory(tower_conv1_1, 256, (3, 1), pad=(1, 0), namepre=namepre+'_tower_conv1_2', args=args['tower_conv1_2'])\n",
    "  tower_mixed = mx.symbol.Concat(*[tower_conv, tower_conv1_2])\n",
    "  tower_out, args['tower_out'] = ConvFactory(\n",
    "      tower_mixed, input_num_channels, (1, 1), with_act=False, namepre=namepre+'_tower_out', args=args['tower_out'])\n",
    "  net = net + scale * tower_out\n",
    "  act = net\n",
    "  if with_act:\n",
    "      act = mx.symbol.Activation(\n",
    "          data=net, act_type=act_type, attr=mirror_attr, name=namepre+'_act')\n",
    "  return act, args\n",
    "\n",
    "\n",
    "def repeat(inputs, repetitions, layer, *ltargs, **kwargs):\n",
    "  outputs = inputs\n",
    "  namepre = kwargs['namepre']\n",
    "  args = kwargs['args']\n",
    "  if args is None:\n",
    "    args = {}\n",
    "    for i in xrange(repetitions):\n",
    "      argname='repeat_'+str(i)\n",
    "      args[argname] = None\n",
    "  for i in range(repetitions):\n",
    "    kwargs['namepre'] = namepre+'_'+str(i)\n",
    "    argname='repeat_'+str(i)\n",
    "    kwargs['args'] = args[argname]\n",
    "#    print ltargs\n",
    "#    print kwargs\n",
    "    outputs, args[argname] = layer(outputs, *ltargs, **kwargs)\n",
    "\n",
    "  return outputs, args\n",
    "\n",
    "\n",
    "def create_inception_resnet_v2(data, namepre='', args=None):\n",
    "  if args is None:\n",
    "    args = {'stem':None, 'reductionA':None, 'repeat_block35':None, 'reductionB':None, \n",
    "            'repeat_block17':None, 'reductionC':None, 'repeat_block8':None, \n",
    "            'final_block8':None, 'final_conv':None, 'finalfc':None}\n",
    "\n",
    "  stem_net, args['stem']= stem(data, namepre=namepre+'_stem', args=args['stem'])\n",
    "\n",
    "  reduceA, args['reductionA'] = reductionA(stem_net, namepre=namepre+'_reductionA', args=args['reductionA'])\n",
    "\n",
    "  repeat_block35, args['repeat_block35'] = repeat(reduceA, 2, block35, scale=0.17, input_num_channels=320, namepre=namepre+'_repeat_block35', args=args['repeat_block35'])\n",
    "\n",
    "\n",
    "  reduceB, args['reductionB'] = reductionB(repeat_block35, namepre=namepre+'_reductionB', args=args['reductionB'])\n",
    "\n",
    "  repeat_block17, args['repeat_block17'] = repeat(reduceB, 4, block17, scale=0.1, input_num_channels=1088, namepre=namepre+'_repeat_block17', args=args['repeat_block17'])\n",
    "\n",
    "  reduceC, args['reductionC'] = reductionC(repeat_block17, namepre=namepre+'_reductionC', args=args['reductionC'])\n",
    "\n",
    "  repeat_block8, args['repeat_block8'] = repeat(reduceC, 2, block8, scale=0.2, input_num_channels=2080, namepre=namepre+'_repeat_block8', args=args['repeat_block8'])\n",
    "  final_block8, args['final_block8'] = block8(repeat_block8, with_act=False, input_num_channels=2080, namepre=namepre+'_final_block8', args=args['final_block8'])\n",
    "\n",
    "  final_conv, args['final_conv'] = ConvFactory(final_block8, 1536, (1, 1), namepre=namepre+'_final_conv', args=args['final_conv'])\n",
    "  final_pool = mx.symbol.Pooling(final_conv, kernel=(8, 8), global_pool=True, pool_type='avg', name=namepre+'_final_pool')\n",
    "#   final_pool = mx.symbol.Pooling(final_conv, kernel=(5, 5), stride=(1, 1), pool_type='avg', name=namepre+'_final_pool')\n",
    "  final_flatten = mx.symbol.Flatten(final_pool, name=namepre+'_final_flatten')\n",
    "\n",
    "  drop1 = mx.sym.Dropout(data=final_flatten, p=0.5, name=namepre+'_dropout1')\n",
    "\n",
    "  if args['finalfc'] is None:\n",
    "    args['finalfc'] = {}\n",
    "    args['finalfc']['weight'] = mx.sym.Variable(namepre+'_fc1_weight')\n",
    "    args['finalfc']['bias'] = mx.sym.Variable(namepre+'_fc1_bias')\n",
    "    \n",
    "  reid_fc1 = mx.sym.FullyConnected(data=drop1, num_hidden=128, name=namepre+\"_fc1\", \n",
    "                                   weight=args['finalfc']['weight'], bias=args['finalfc']['bias']) \n",
    "#  reid_act = mx.sym.Activation(data=reid_fc1, act_type='tanh', name=namepre+'_fc1_relu')\n",
    "\n",
    "  net = reid_fc1\n",
    "#  net = final_flatten\n",
    "\n",
    "  return net, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net(data, radius):\n",
    "#     data = mx.sym.Variable('data')\n",
    "    args_all = None\n",
    "    feat_final, args_all = create_inception_resnet_v2(data, namepre='part1', args=args_all)\n",
    "    feat_final = mx.sym.BatchNorm(data=feat_final, fix_gamma=False, name='feat_bn1')\n",
    "\n",
    "\n",
    "    min_value =10**-36\n",
    "    norm_value = radius\n",
    "    #  norm_value = 24\n",
    "    logging.info('norm_value:%f, min_value:%e, hardratio:%f', norm_value, min_value, hardratio)\n",
    "\n",
    "    #norm\n",
    "    znorm_loss = None\n",
    "    if norm_value>0:\n",
    "    #    proxy_Z = mx.sym.L2Normalization(proxy_Z) * norm_value\n",
    "    #    feat_final = mx.sym.L2Normalization(feat_final) * norm_value\n",
    "#         proxy_Znorm = mx.sym.sum_axis(proxy_Z**2, axis=1)\n",
    "#         proxy_Znorm = mx.sym.sqrt(proxy_Znorm) + min_value\n",
    "\n",
    "#     #    znorm_loss = mx.sym.abs(proxy_Znorm - 1.0)\n",
    "#     #    znorm_loss = mx.sym.sum(znorm_loss)\n",
    "#     #    znorm_loss = mx.sym.MakeLoss(znorm_loss)\n",
    "\n",
    "#         proxy_Znorm = mx.sym.Reshape(proxy_Znorm, shape=(-2, 1))\n",
    "#         proxy_Z = mx.sym.broadcast_div(proxy_Z, proxy_Znorm)# * norm_value\n",
    "\n",
    "        feat_finalnorm = mx.sym.sum_axis(feat_final**2, axis=1)\n",
    "        feat_finalnorm = mx.sym.sqrt(feat_finalnorm) + min_value\n",
    "        feat_finalnorm = mx.sym.Reshape(feat_finalnorm, shape=(-2, 1))\n",
    "        feat_final = mx.sym.broadcast_div(feat_final, feat_finalnorm) * norm_value\n",
    "#     X = mx.nd.empty(shape=(64, 10000000), ctx=mx.cpu(0))\n",
    "    return feat_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define module and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixModule(object):\n",
    "    def __init__(self, symbol, context, handle, data_shape, proxy_Z, K):\n",
    "        self.mod = mx.mod.Module(symbol=symbol, \n",
    "                                 data_names=(\"data\",), \n",
    "                                 label_names=None, \n",
    "                                 context=context)\n",
    "        self.mod.bind(data_shapes=[(\"data\", data_shape)])\n",
    "        self.context = context if isinstance(context, list) else [context]\n",
    "        self.handle = handle\n",
    "        self.W = proxy_Z\n",
    "        self.N = data_shape[0]\n",
    "        self.C, self.M = self.W.shape\n",
    "        self.score = np.empty((self.N, self.M), np.float32)\n",
    "        self.X = np.zeros((self.N, self.C), np.float32)\n",
    "        self.K = K\n",
    "        self.history = mx.nd.empty(shape=(self.C, self.M), ctx=mx.cpu(0), dtype='float32')\n",
    "    def init_params(self, *args, **kwargs):\n",
    "        self.mod.init_params(*args, **kwargs)\n",
    "\n",
    "    def init_optimizer(self, *args, **kwargs):\n",
    "        self.mod.init_optimizer(*args, **kwargs)\n",
    "        \n",
    "    def update(self, data_batch):\n",
    "        self.mod.forward(data_batch)\n",
    "        self.X = self.mod.get_outputs()[0].asnumpy()\n",
    "        y_true = data_batch.label[0].asnumpy().reshape(-1,1)\n",
    "        \n",
    "        def stream_cal_in_GPU():        \n",
    "            cublasxt.cublasXtSgemm(self.handle,\n",
    "                                   'N', 'N', \n",
    "                                   self.N, self.M, self.C, np.float32(1.0),\n",
    "                                   self.X.ctypes.data, self.N, self.W.ctypes.data, self.C, np.float32(0.0),\n",
    "                                   self.score.ctypes.data, self.N)\n",
    "        cublasxt.cublasXtSetCpuRoutine(self.handle, 0, 0, stream_cal_in_GPU())\n",
    "        \n",
    "        self.score = self.score - np.max(self.score, axis=1, keepdims=True)\n",
    "        self.score = np.exp(self.score)\n",
    "        self.score = self.score / np.sum(self.score, axis=1, keepdims=True)\n",
    "        print self.score.shape\n",
    "        y_true_score = self.score[range(self.N), y_true.reshape(-1)]\n",
    "        self.score[range(self.N), y_true.reshape(-1)] = self.score.min(axis = 1)\n",
    "        \n",
    "        TopK_idx = np.argpartition(self.score, -self.K, axis=1)[:, -self.K:]\n",
    "        self.score[range(self.N), y_true.reshape(-1)] = y_true_score\n",
    "        TopK_idx = np.hstack((TopK_idx, y_true))\n",
    "        \n",
    "        TopK_score = self.score[np.array(range(self.N)).reshape(-1,1), TopK_idx]\n",
    "        y_true_reform = np.array([self.K]*self.N).reshape(-1,1)\n",
    "        \n",
    "        def softmax_loss(sparse_probs, y_true):\n",
    "            \"\"\"\n",
    "            Computes the loss and gradient for softmax classification.\n",
    "            Inputs:\n",
    "            - sparse_probs: Input data, of shape (N, K) where sparse_probs[i, j] is the probability for the jth\n",
    "              class for the ith input.\n",
    "            - y_true: Vector of labels, of shape (N,1) where y_true[i] is the label for probs[i] and\n",
    "              0 <= y_true[i] < K\n",
    "            Returns a tuple of:\n",
    "            - loss: Scalar giving the loss\n",
    "            - d_sparse_score: shape: (N, K), Gradient of the loss with respect to sparse_score (not sparse_probs)\n",
    "            \"\"\"\n",
    "            N = sparse_probs.shape[0]\n",
    "        #     # Numerical stability\n",
    "        #     shifted_sparse_score = sparse_score - np.max(sparse_score, axis=1, keepdims=True)\n",
    "\n",
    "        #     Z = np.sum(np.exp(shifted_sparse_score), axis=1, keepdims=True)\n",
    "        #     log_probs = shifted_sparse_score - np.log(Z)\n",
    "        #     probs = np.exp(log_probs)\n",
    "            log_sparse_probs = np.log(sparse_probs)\n",
    "            loss = -np.sum(log_sparse_probs[np.arange(N), y_true.reshape(-1)]) / N\n",
    "            d_sparse_score = sparse_probs.copy()\n",
    "            d_sparse_score[np.arange(N), y_true.reshape(-1)] -= 1\n",
    "            # rescale gradient\n",
    "            d_sparse_score /= N\n",
    "            return loss, d_sparse_score\n",
    "        \n",
    "        loss, d_TopK_score = softmax_loss(TopK_score, y_true_reform)\n",
    "        self.score[...] = 0\n",
    "        self.score[np.array(range(self.N)).reshape(-1,1), TopK_idx] = d_TopK_score\n",
    "        d_score = mx.nd.array(self.score, dtype='float32')\n",
    "        csr_d_score = d_score.tostype('csr')\n",
    "        \n",
    "        feat_final = mx.nd.array(self.X, dtype='float32', ctx=mx.cpu(0))\n",
    "        proxy_Z = mx.nd.array(self.W, dtype='float32', ctx=mx.cpu(0))\n",
    "        \n",
    "        d_proxy_Z = mx.ndarray.sparse.dot(feat_final.T, csr_d_score)\n",
    "#         print d_proxy_Z.shape\n",
    "        d_feat_final = mx.nd.dot(csr_d_score, proxy_Z.T)\n",
    "        # backprop\n",
    "        self.mod.backward([d_feat_final])\n",
    "\n",
    "        # update W\n",
    "        lr = self.mod._optimizer._get_lr(self.mod._optimizer.num_update)\n",
    "        wd = self.mod._optimizer._get_wd(self.mod._optimizer.num_update)\n",
    "        eps = self.mod._optimizer.epsilon\n",
    "        print 'iter: %d, lr: %e' % (self.mod._optimizer.num_update, lr)\n",
    "        self.history[:] += (d_proxy_Z**2)\n",
    "        proxy_Z[:] += -lr * (d_proxy_Z / (self.history + eps).sqrt() + wd * proxy_Z)\n",
    "        self.W = proxy_Z.asnumpy()\n",
    "        # update module\n",
    "        self.mod.update()\n",
    "        \n",
    "        # save proxy_Z(W) after a certain number of self.mod._optimizer.num_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone + fc + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size per device: 64\n",
      "lr_start:6.0e-02, lr_min:1.0e-05, lr_reduce:0.96, lr_stepsnum:214\n"
     ]
    }
   ],
   "source": [
    "ctx = [mx.gpu(0)]\n",
    "handle = cublasxt.cublasXtCreate()\n",
    "# mode = cublasxt.cublasXtGetPinningMemMode(handle)\n",
    "cublasxt.cublasXtSetPinningMemMode(handle, 1)\n",
    "cublasxt.cublasXtSetCpuRatio(handle, 0, 0, 0.9)\n",
    "nbDevices = len(ctx)\n",
    "deviceId = np.array(range(nbDevices), np.int32)\n",
    "cublasxt.cublasXtDeviceSelect(handle, nbDevices, deviceId)\n",
    "\n",
    "num_epoch = 1000000\n",
    "batch_size = 64*nbDevices\n",
    "show_period = 1000\n",
    "\n",
    "assert(batch_size%nbDevices==0)\n",
    "bsz_per_device = batch_size / nbDevices\n",
    "print 'batch_size per device:', bsz_per_device\n",
    "\n",
    "featdim = 128\n",
    "total_proxy_num = 300000\n",
    "data_shape = (batch_size, 3, 240, 120)\n",
    "proxy_Z_shape = (total_proxy_num, featdim)\n",
    "proxy_Z_fn = './proxy_Z.params'\n",
    "proxy_Ztmp = np.random.rand(featdim, total_proxy_num)-0.5\n",
    "proxy_Z = proxy_Ztmp.astype(np.float32)\n",
    "\n",
    "if os.path.exists(proxy_Z_fn):\n",
    "    tmpZ = mx.nd.load(proxy_Z_fn)\n",
    "    proxy_Z = tmpZ[0].asnumpy()\n",
    "    print proxy_num, tmpZ[0].shape[0]\n",
    "    assert(proxy_num==tmpZ[0].shape[0])\n",
    "    print 'load proxy_Z from', proxy_Z_fn\n",
    "\n",
    "dlr = 1050/batch_size*2\n",
    "radius = 32\n",
    "hardratio = 10**-5\n",
    "lr_start = 0.06\n",
    "lr_min = 10**-5\n",
    "lr_reduce = 0.96 #0.99\n",
    "lr_stepnum = np.log(lr_min/lr_start)/np.log(lr_reduce)\n",
    "lr_stepnum = np.int(np.ceil(lr_stepnum))\n",
    "dlr_steps = [dlr*i for i in xrange(1, lr_stepnum+1)]\n",
    "print 'lr_start:%.1e, lr_min:%.1e, lr_reduce:%.2f, lr_stepsnum:%d'%(lr_start, lr_min, lr_reduce, lr_stepnum)\n",
    "#     print dlr_steps\n",
    "lr_scheduler = mx.lr_scheduler.MultiFactorScheduler(dlr_steps, lr_reduce)\n",
    "\n",
    "# param_prefix = 'MDL_PARAM/params5_proxy_nca-8wmargin_20180724_dim_512_bn/person_reid-back'\n",
    "param_prefix = './'\n",
    "load_paramidx = 0 #None\n",
    "\n",
    "# simple DataBatch test\n",
    "data_batch = mx.random.normal(0, 1.0, shape=data_shape)\n",
    "data_label = mx.nd.array(range(64), dtype='int32')\n",
    "DataIter = mx.io.DataBatch(data=[data_batch], label=[data_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mxmod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-03ce2f0e7ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmxmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mxmod' is not defined"
     ]
    }
   ],
   "source": [
    "del mxmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#     ctxs = [mx.gpu(0), mx.gpu(1), mx.gpu(2)]\n",
    "\n",
    "data = mx.sym.Variable('data')\n",
    "part_net = create_net(data, radius)\n",
    "mxmod = mixModule(symbol=part_net, \n",
    "                context=ctx, \n",
    "                handle=handle, \n",
    "                data_shape=data_shape, \n",
    "                proxy_Z=proxy_Z, \n",
    "                K = 999)\n",
    "\n",
    "mxmod.init_params(mx.init.Xavier(factor_type=\"in\", magnitude=2.34))\n",
    "mxmod.init_optimizer(optimizer=\"adam\", \n",
    "                   optimizer_params={\n",
    "                       \"learning_rate\": lr_start,\n",
    "                       'lr_scheduler':lr_scheduler,\n",
    "                       'clip_gradient':None,\n",
    "                       \"wd\": 0.0005,\n",
    "#                            \"beta1\": beta1,\n",
    "                   })\n",
    "# mod.update(DataIter, is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 300000)\n",
      "iter: 0, lr: 6.000000e-02\n"
     ]
    }
   ],
   "source": [
    "mxmod.update(DataIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 300000)\n",
      "iter: 1, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 2, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 3, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 4, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 5, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 6, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 7, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 8, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 9, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 10, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 11, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 12, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 13, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 14, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 15, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 16, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 17, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 18, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 19, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 20, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 21, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 22, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 23, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 24, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 25, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 26, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 27, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 28, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 29, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 30, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 31, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 32, lr: 6.000000e-02\n",
      "(64, 300000)\n",
      "iter: 33, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 34, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 35, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 36, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 37, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 38, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 39, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 40, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 41, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 42, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 43, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 44, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 45, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 46, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 47, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 48, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 49, lr: 5.760000e-02\n",
      "(64, 300000)\n",
      "iter: 50, lr: 5.760000e-02\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    mxmod.update(DataIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
